{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0JhrX0/VRzPKhByu5MPRY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayankcharde/Fire-Smoke-Detection-repo/blob/main/SmokeFireDetection(Vedio).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qDQEOp2UAJIi",
        "outputId": "98bfbb87-0223-4836-a9ca-80f5f25432aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CUDA available: False\n",
            "Files in /content: ['.config', 'bucket11 (1).mp4', 'yolov8n.pt', 'output_detection_fast.mp4', 'bucket11.mp4', 'markup.json', 'printer31.mp4', 'yolo_fire_smoke', 'sample_data']\n",
            "Videos in markup: ['bench11', 'bench12', 'bench21', 'bench22', 'bench31', 'boiler11', 'boiler12', 'boiler21', 'boiler22', 'boiler31', 'boiler32', 'bucket11', 'bucket12', 'bucket22', 'bucket23', 'bucket31', 'bucket32', 'bucket33', 'bucket42', 'corridor11', 'corridor21', 'corridor22', 'door11', 'door22', 'door33', 'door41', 'door52', 'door63', 'door74', 'fire11', 'fire22', 'fire31', 'fire42', 'gas11', 'gas12', 'gas21', 'gas22', 'gas31', 'gas32', 'office11', 'office12', 'office21', 'office22', 'office31', 'office42', 'officebucket11', 'officebucket21', 'officebucket22', 'officebucket31', 'officebucket32', 'printer11', 'printer12', 'printer21', 'printer22', 'printer31', 'printer32', 'roomfire11', 'roomfire22', 'roomfire31', 'roomfire41', 'roomsmoke11', 'roomsmoke22', 'roomsmoke32', 'roomsmoke42', 'roomsmoke53', 'smoking11', 'smoking12', 'smoking21', 'smoking22', 'smoking31', 'smoking32', 'store11', 'store12', 'store21', 'store22', 'wire11', 'wire12', 'wire21', 'wire22', 'yard11', 'yard12', 'yard21', 'yard22', 'yard31', 'yard32']\n",
            "Excluded from training: {'roomfire41'}\n",
            "Class mapping: {'fire': 0, 'smoke': 1}\n",
            "[WARN] Missing video file for key 'bench11': /content/bench11.mp4\n",
            "[WARN] Missing video file for key 'bench12': /content/bench12.mp4\n",
            "[WARN] Missing video file for key 'bench21': /content/bench21.mp4\n",
            "[WARN] Missing video file for key 'bench22': /content/bench22.mp4\n",
            "[WARN] Missing video file for key 'bench31': /content/bench31.mp4\n",
            "[WARN] Missing video file for key 'boiler11': /content/boiler11.mp4\n",
            "[WARN] Missing video file for key 'boiler12': /content/boiler12.mp4\n",
            "[WARN] Missing video file for key 'boiler21': /content/boiler21.mp4\n",
            "[WARN] Missing video file for key 'boiler22': /content/boiler22.mp4\n",
            "[WARN] Missing video file for key 'boiler31': /content/boiler31.mp4\n",
            "[WARN] Missing video file for key 'boiler32': /content/boiler32.mp4\n",
            "\n",
            "Processing /content/bucket11.mp4 with 76 annotated frames...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "bucket11 frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76/76 [00:12<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Missing video file for key 'bucket12': /content/bucket12.mp4\n",
            "[WARN] Missing video file for key 'bucket22': /content/bucket22.mp4\n",
            "[WARN] Missing video file for key 'bucket23': /content/bucket23.mp4\n",
            "[WARN] Missing video file for key 'bucket31': /content/bucket31.mp4\n",
            "[WARN] Missing video file for key 'bucket32': /content/bucket32.mp4\n",
            "[WARN] Missing video file for key 'bucket33': /content/bucket33.mp4\n",
            "[WARN] Missing video file for key 'bucket42': /content/bucket42.mp4\n",
            "[WARN] Missing video file for key 'corridor11': /content/corridor11.mp4\n",
            "[WARN] Missing video file for key 'corridor21': /content/corridor21.mp4\n",
            "[WARN] Missing video file for key 'corridor22': /content/corridor22.mp4\n",
            "[WARN] Missing video file for key 'door11': /content/door11.mp4\n",
            "[WARN] Missing video file for key 'door22': /content/door22.mp4\n",
            "[WARN] Missing video file for key 'door33': /content/door33.mp4\n",
            "[WARN] Missing video file for key 'door41': /content/door41.mp4\n",
            "[WARN] Missing video file for key 'door52': /content/door52.mp4\n",
            "[WARN] Missing video file for key 'door63': /content/door63.mp4\n",
            "[WARN] Missing video file for key 'door74': /content/door74.mp4\n",
            "[WARN] Missing video file for key 'fire11': /content/fire11.mp4\n",
            "[WARN] Missing video file for key 'fire22': /content/fire22.mp4\n",
            "[WARN] Missing video file for key 'fire31': /content/fire31.mp4\n",
            "[WARN] Missing video file for key 'fire42': /content/fire42.mp4\n",
            "[WARN] Missing video file for key 'gas11': /content/gas11.mp4\n",
            "[WARN] Missing video file for key 'gas12': /content/gas12.mp4\n",
            "[WARN] Missing video file for key 'gas21': /content/gas21.mp4\n",
            "[WARN] Missing video file for key 'gas22': /content/gas22.mp4\n",
            "[WARN] Missing video file for key 'gas31': /content/gas31.mp4\n",
            "[WARN] Missing video file for key 'gas32': /content/gas32.mp4\n",
            "[WARN] Missing video file for key 'office11': /content/office11.mp4\n",
            "[WARN] Missing video file for key 'office12': /content/office12.mp4\n",
            "[WARN] Missing video file for key 'office21': /content/office21.mp4\n",
            "[WARN] Missing video file for key 'office22': /content/office22.mp4\n",
            "[WARN] Missing video file for key 'office31': /content/office31.mp4\n",
            "[WARN] Missing video file for key 'office42': /content/office42.mp4\n",
            "[WARN] Missing video file for key 'officebucket11': /content/officebucket11.mp4\n",
            "[WARN] Missing video file for key 'officebucket21': /content/officebucket21.mp4\n",
            "[WARN] Missing video file for key 'officebucket22': /content/officebucket22.mp4\n",
            "[WARN] Missing video file for key 'officebucket31': /content/officebucket31.mp4\n",
            "[WARN] Missing video file for key 'officebucket32': /content/officebucket32.mp4\n",
            "[WARN] Missing video file for key 'printer11': /content/printer11.mp4\n",
            "[WARN] Missing video file for key 'printer12': /content/printer12.mp4\n",
            "[WARN] Missing video file for key 'printer21': /content/printer21.mp4\n",
            "[WARN] Missing video file for key 'printer22': /content/printer22.mp4\n",
            "\n",
            "Processing /content/printer31.mp4 with 119 annotated frames...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "printer31 frames: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 119/119 [00:17<00:00,  6.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Missing video file for key 'printer32': /content/printer32.mp4\n",
            "[WARN] Missing video file for key 'roomfire11': /content/roomfire11.mp4\n",
            "[WARN] Missing video file for key 'roomfire22': /content/roomfire22.mp4\n",
            "[WARN] Missing video file for key 'roomfire31': /content/roomfire31.mp4\n",
            "\n",
            "[INFO] Skipping roomfire41 (excluded from training)\n",
            "[WARN] Missing video file for key 'roomsmoke11': /content/roomsmoke11.mp4\n",
            "[WARN] Missing video file for key 'roomsmoke22': /content/roomsmoke22.mp4\n",
            "[WARN] Missing video file for key 'roomsmoke32': /content/roomsmoke32.mp4\n",
            "[WARN] Missing video file for key 'roomsmoke42': /content/roomsmoke42.mp4\n",
            "[WARN] Missing video file for key 'roomsmoke53': /content/roomsmoke53.mp4\n",
            "[WARN] Missing video file for key 'smoking11': /content/smoking11.mp4\n",
            "[WARN] Missing video file for key 'smoking12': /content/smoking12.mp4\n",
            "[WARN] Missing video file for key 'smoking21': /content/smoking21.mp4\n",
            "[WARN] Missing video file for key 'smoking22': /content/smoking22.mp4\n",
            "[WARN] Missing video file for key 'smoking31': /content/smoking31.mp4\n",
            "[WARN] Missing video file for key 'smoking32': /content/smoking32.mp4\n",
            "[WARN] Missing video file for key 'store11': /content/store11.mp4\n",
            "[WARN] Missing video file for key 'store12': /content/store12.mp4\n",
            "[WARN] Missing video file for key 'store21': /content/store21.mp4\n",
            "[WARN] Missing video file for key 'store22': /content/store22.mp4\n",
            "[WARN] Missing video file for key 'wire11': /content/wire11.mp4\n",
            "[WARN] Missing video file for key 'wire12': /content/wire12.mp4\n",
            "[WARN] Missing video file for key 'wire21': /content/wire21.mp4\n",
            "[WARN] Missing video file for key 'wire22': /content/wire22.mp4\n",
            "[WARN] Missing video file for key 'yard11': /content/yard11.mp4\n",
            "[WARN] Missing video file for key 'yard12': /content/yard12.mp4\n",
            "[WARN] Missing video file for key 'yard21': /content/yard21.mp4\n",
            "[WARN] Missing video file for key 'yard22': /content/yard22.mp4\n",
            "[WARN] Missing video file for key 'yard31': /content/yard31.mp4\n",
            "[WARN] Missing video file for key 'yard32': /content/yard32.mp4\n",
            "\n",
            "Total samples after subsampling: 195\n",
            "Train samples: 156 | Val samples: 39\n",
            "\n",
            "YOLO data config:\n",
            "\n",
            "path: /content/yolo_fire_smoke\n",
            "train: images/train\n",
            "val: images/val\n",
            "names:\n",
            "  0: fire\n",
            "  1: smoke\n",
            "\n",
            "Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/yolo_fire_smoke/fire_smoke.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fire_smoke_fast, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/yolo_fire_smoke, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/yolo_fire_smoke/fire_smoke_fast, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=2, workspace=None\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "Model summary: 129 layers, 3,011,238 parameters, 3,011,222 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1053.1¬±296.1 MB/s, size: 68.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_fire_smoke/labels/train... 156 images, 33 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 156/156 2.3Kit/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_fire_smoke/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 893.1¬±341.8 MB/s, size: 65.4 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_fire_smoke/labels/val... 39 images, 16 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 39/39 2.2Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_fire_smoke/labels/val.cache\n",
            "Plotting labels to /content/yolo_fire_smoke/fire_smoke_fast/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 512 train, 512 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/yolo_fire_smoke/fire_smoke_fast\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/10         0G      2.756      5.986      2.218          3        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.2s/it 1:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.6s/it 7.8s\n",
            "                   all         39         29   0.000977      0.271     0.0164    0.00767\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/10         0G      2.053      3.802      1.681          5        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.7s\n",
            "                   all         39         29    0.00149      0.512     0.0588     0.0257\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/10         0G      1.884      3.278      1.605          3        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.6s\n",
            "                   all         39         29      0.813     0.0417      0.353      0.223\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/10         0G      1.696      3.152      1.469          2        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.6s\n",
            "                   all         39         29      0.639      0.525      0.578      0.301\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/10         0G      1.669      2.902      1.491          4        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.7s\n",
            "                   all         39         29       0.56      0.595      0.606      0.356\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/10         0G      1.721      2.912      1.508          4        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.6s\n",
            "                   all         39         29        0.7      0.667      0.644       0.41\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/10         0G      1.586      2.534      1.465          6        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.7s/it 8.1s\n",
            "                   all         39         29      0.709      0.729      0.726      0.403\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/10         0G      1.611      2.447       1.49          4        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.8s/it 8.5s\n",
            "                   all         39         29      0.899      0.625      0.735      0.415\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/10         0G       1.55      2.279      1.424          4        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:23\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 3.0s/it 8.9s\n",
            "                   all         39         29       0.67       0.75      0.781      0.458\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/10         0G      1.437      2.132      1.349          5        512: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 20/20 4.1s/it 1:22\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.9s/it 8.6s\n",
            "                   all         39         29      0.723       0.75      0.791      0.479\n",
            "\n",
            "10 epochs completed in 0.253 hours.\n",
            "Optimizer stripped from /content/yolo_fire_smoke/fire_smoke_fast/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/yolo_fire_smoke/fire_smoke_fast/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/yolo_fire_smoke/fire_smoke_fast/weights/best.pt...\n",
            "Ultralytics 8.3.233 üöÄ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3/3 2.7s/it 8.2s\n",
            "                   all         39         29      0.722       0.75      0.791      0.478\n",
            "                  fire          5          5       0.88          1      0.995      0.649\n",
            "                 smoke         20         24      0.563        0.5      0.587      0.308\n",
            "Speed: 2.5ms preprocess, 178.2ms inference, 0.0ms loss, 20.5ms postprocess per image\n",
            "Results saved to \u001b[1m/content/yolo_fire_smoke/fire_smoke_fast\u001b[0m\n",
            "\n",
            "Best model weights at: /content/yolo_fire_smoke/fire_smoke_fast/weights/best.pt\n",
            "\n",
            "Upload a video for detection (e.g., roomfire41.mp4):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dd026084-8eb6-4d6a-80d0-c19f93c40b69\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dd026084-8eb6-4d6a-80d0-c19f93c40b69\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving printer31.mp4 to printer31 (1).mp4\n",
            "Running detection on: /content/printer31 (1).mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-593449927.py:224: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  c = int(box.cls.cpu().numpy())\n",
            "/tmp/ipython-input-593449927.py:225: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  conf = float(box.conf.cpu().numpy())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Fast detection video saved at: /content/output_detection_fast.mp4\n",
            "Download or play it from the Files panel.\n"
          ]
        }
      ],
      "source": [
        "# ===========================================================\n",
        "# FAST FIRE & SMOKE TRAINING + VIDEO PREDICTION (YOLOv8n)\n",
        "# - Uses /content/bucket11.mp4, /content/printer31.mp4\n",
        "# - Skips roomfire41.mp4 for training\n",
        "# - Uses /content/markup.json\n",
        "# - Downsamples frames, smaller imgsz, fewer epochs\n",
        "# ===========================================================\n",
        "\n",
        "!pip install ultralytics opencv-python tqdm --quiet\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "from shutil import copy2\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import torch\n",
        "\n",
        "print(\"PyTorch CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# ------------------ FAST TRAINING HYPERPARAMS ------------------\n",
        "IMG_SIZE = 512          # smaller than 640 -> faster\n",
        "EPOCHS = 10             # fewer epochs -> faster\n",
        "BATCH_SIZE = 8\n",
        "MAX_FRAMES_PER_VIDEO = 300   # limit annotated frames per video\n",
        "EXCLUDED_VIDEOS = {\"roomfire41\"}   # not used for training\n",
        "CLASS_MAP = {\"fire\": 0, \"smoke\": 1}\n",
        "\n",
        "# ===========================================================\n",
        "# 1Ô∏è‚É£ Paths\n",
        "# ===========================================================\n",
        "DATA_DIR   = \"/content\"\n",
        "VIDEO_DIR  = \"/content\"\n",
        "MARKUP_PATH = \"/content/markup.json\"\n",
        "\n",
        "print(\"Files in /content:\", os.listdir(\"/content\"))\n",
        "if not os.path.exists(MARKUP_PATH):\n",
        "    raise FileNotFoundError(\"markup.json not found in /content. Upload it first.\")\n",
        "\n",
        "# Clean previous dataset folder if exists\n",
        "YOLO_DATA_DIR = \"/content/yolo_fire_smoke\"\n",
        "if os.path.exists(YOLO_DATA_DIR):\n",
        "    shutil.rmtree(YOLO_DATA_DIR)\n",
        "\n",
        "IMAGES_DIR = os.path.join(YOLO_DATA_DIR, \"images\")\n",
        "LABELS_DIR = os.path.join(YOLO_DATA_DIR, \"labels\")\n",
        "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
        "os.makedirs(LABELS_DIR, exist_ok=True)\n",
        "\n",
        "# ===========================================================\n",
        "# 2Ô∏è‚É£ Load markup.json\n",
        "# ===========================================================\n",
        "with open(MARKUP_PATH, \"r\") as f:\n",
        "    markup = json.load(f)\n",
        "\n",
        "print(\"Videos in markup:\", list(markup.keys()))\n",
        "print(\"Excluded from training:\", EXCLUDED_VIDEOS)\n",
        "print(\"Class mapping:\", CLASS_MAP)\n",
        "\n",
        "def extract_frame(cap, frame_idx):\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "    ret, frame = cap.read()\n",
        "    return frame if ret else None\n",
        "\n",
        "# ===========================================================\n",
        "# 3Ô∏è‚É£ Build YOLO dataset (with FRAME SUBSAMPLING)\n",
        "# ===========================================================\n",
        "all_samples = []\n",
        "\n",
        "for video_key, frames in markup.items():\n",
        "    if video_key in EXCLUDED_VIDEOS:\n",
        "        print(f\"\\n[INFO] Skipping {video_key} (excluded from training)\")\n",
        "        continue\n",
        "\n",
        "    video_path = os.path.join(VIDEO_DIR, f\"{video_key}.mp4\")\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"[WARN] Missing video file for key '{video_key}': {video_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {video_path} with {len(frames)} annotated frames...\")\n",
        "\n",
        "    # ---- Subsample frames for speed ----\n",
        "    if len(frames) > MAX_FRAMES_PER_VIDEO:\n",
        "        frames = random.sample(frames, MAX_FRAMES_PER_VIDEO)\n",
        "        print(f\"[INFO] Subsampled to {len(frames)} frames for faster training\")\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    for f in tqdm(frames, desc=f\"{video_key} frames\"):\n",
        "        w, h = f[\"width\"], f[\"height\"]\n",
        "        frame_num = f[\"frame_num\"]\n",
        "\n",
        "        frame = extract_frame(cap, frame_num)\n",
        "        if frame is None:\n",
        "            continue\n",
        "\n",
        "        img_name = f\"{video_key}_frame{frame_num}.jpg\"\n",
        "        img_path = os.path.join(IMAGES_DIR, img_name)\n",
        "\n",
        "        # Optional: resize frame down before saving (YOLO will resize again,\n",
        "        # but this saves disk & IO, makes things a bit lighter)\n",
        "        frame_resized = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
        "        cv2.imwrite(img_path, frame_resized)\n",
        "\n",
        "        label_path = os.path.join(LABELS_DIR, img_name.replace(\".jpg\", \".txt\"))\n",
        "        lines = []\n",
        "\n",
        "        for obj in f[\"objects\"]:\n",
        "            cls = CLASS_MAP.get(obj[\"class\"], None)\n",
        "            if cls is None:\n",
        "                continue\n",
        "\n",
        "            x1, y1, x2, y2 = obj[\"x1\"], obj[\"y1\"], obj[\"x2\"], obj[\"y2\"]\n",
        "\n",
        "            # NOTE: normalization still uses original w, h from markup\n",
        "            xc = (x1 + x2) / 2 / w\n",
        "            yc = (y1 + y2) / 2 / h\n",
        "            bw = (x2 - x1) / w\n",
        "            bh = (y2 - y1) / h\n",
        "\n",
        "            lines.append(f\"{cls} {xc} {yc} {bw} {bh}\")\n",
        "\n",
        "        with open(label_path, \"w\") as lf:\n",
        "            lf.write(\"\\n\".join(lines))\n",
        "\n",
        "        all_samples.append((img_path, label_path))\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "print(\"\\nTotal samples after subsampling:\", len(all_samples))\n",
        "if len(all_samples) == 0:\n",
        "    raise RuntimeError(\"No samples created. Check markup.json and filenames.\")\n",
        "\n",
        "# ===========================================================\n",
        "# 4Ô∏è‚É£ Train/Val split\n",
        "# ===========================================================\n",
        "random.shuffle(all_samples)\n",
        "split_idx = int(len(all_samples) * 0.8)\n",
        "train_samples = all_samples[:split_idx]\n",
        "val_samples   = all_samples[split_idx:]\n",
        "\n",
        "for split_type in [\"train\", \"val\"]:\n",
        "    os.makedirs(os.path.join(IMAGES_DIR, split_type), exist_ok=True)\n",
        "    os.makedirs(os.path.join(LABELS_DIR, split_type), exist_ok=True)\n",
        "\n",
        "def move_samples(samples, split_name):\n",
        "    for img, label in samples:\n",
        "        copy2(img, os.path.join(IMAGES_DIR, split_name, os.path.basename(img)))\n",
        "        copy2(label, os.path.join(LABELS_DIR, split_name, os.path.basename(label)))\n",
        "\n",
        "move_samples(train_samples, \"train\")\n",
        "move_samples(val_samples, \"val\")\n",
        "\n",
        "print(f\"Train samples: {len(train_samples)} | Val samples: {len(val_samples)}\")\n",
        "\n",
        "# ===========================================================\n",
        "# 5Ô∏è‚É£ Create YOLO data config\n",
        "# ===========================================================\n",
        "yaml_path = os.path.join(YOLO_DATA_DIR, \"fire_smoke.yaml\")\n",
        "with open(yaml_path, \"w\") as f:\n",
        "    f.write(f\"\"\"\n",
        "path: {YOLO_DATA_DIR}\n",
        "train: images/train\n",
        "val: images/val\n",
        "names:\n",
        "  0: fire\n",
        "  1: smoke\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nYOLO data config:\")\n",
        "print(open(yaml_path).read())\n",
        "\n",
        "# ===========================================================\n",
        "# 6Ô∏è‚É£ Train YOLOv8n (fast)\n",
        "# ===========================================================\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "results = model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=EPOCHS,         # faster\n",
        "    imgsz=IMG_SIZE,        # smaller -> faster\n",
        "    batch=BATCH_SIZE,\n",
        "    workers=2,             # small but helps\n",
        "    name=\"fire_smoke_fast\",\n",
        "    project=YOLO_DATA_DIR\n",
        ")\n",
        "\n",
        "MODEL_PATH = os.path.join(YOLO_DATA_DIR, \"fire_smoke_fast\", \"weights\", \"best.pt\")\n",
        "print(\"\\nBest model weights at:\", MODEL_PATH)\n",
        "\n",
        "# ===========================================================\n",
        "# 7Ô∏è‚É£ Detection on any uploaded video (you can use roomfire41.mp4)\n",
        "# ===========================================================\n",
        "print(\"\\nUpload a video for detection (e.g., roomfire41.mp4):\")\n",
        "uploaded = files.upload()\n",
        "test_video = list(uploaded.keys())[0]\n",
        "input_video = f\"/content/{test_video}\"\n",
        "print(\"Running detection on:\", input_video)\n",
        "\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "\n",
        "out_path = \"/content/output_detection_fast.mp4\"\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(out_path, fourcc, fps, (w, h))\n",
        "\n",
        "detect_model = YOLO(MODEL_PATH)\n",
        "colors = {0: (0, 0, 255), 1: (255, 0, 0)}  # fire=red, smoke=blue\n",
        "labels = {0: \"fire\", 1: \"smoke\"}\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    results = detect_model(frame, conf=0.4, verbose=False)[0]\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "        c = int(box.cls.cpu().numpy())\n",
        "        conf = float(box.conf.cpu().numpy())\n",
        "        lbl = f\"{labels.get(c, 'obj')} {conf:.2f}\"\n",
        "        color = colors.get(c, (0, 255, 0))\n",
        "\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)\n",
        "        cv2.putText(frame, lbl, (int(x1), int(y1) - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "print(\"\\n‚úÖ Fast detection video saved at:\", out_path)\n",
        "print(\"Download or play it from the Files panel.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MbmjR_l9CizO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}